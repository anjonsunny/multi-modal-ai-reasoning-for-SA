<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bridge Monitoring System</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body>
    <div class="container my-4">
        <h1 class="text-center">Bridge Monitoring System</h1>
        <p class="lead">
            This project is a real-time bridge monitoring system designed to detect anomalous activity using multi-modal data. It integrates sensor data, video feeds, and object detection (YOLO) to provide situational awareness, reasoning, and meta-reasoning outputs for military operations.
        </p>
        <h2>Features</h2>
        <ul>
            <li>Real-time video feed processing with YOLO object detection.</li>
            <li>Reasoning outputs for situational awareness, enemy intention, and threat levels.</li>
            <li>Meta-reasoning summaries visualized as node clusters, aggregating insights across multiple chunks.</li>
            <li>Live frame lag visualization and caption generation status.</li>
        </ul>
        <h2>Screenshot</h2>
        <img src="meta-reasoning-screenshot.png" alt="Meta-Reasoning Panel" class="img-fluid mb-3">
        <h2>Technical Details</h2>
        <p>
            Built using Python and Dash, this system leverages LLMs (e.g., Ollama) for reasoning and meta-reasoning, processing data in real-time to provide actionable recommendations. The UI includes interactive panels for reasoning outputs, meta-reasoning clusters, and historical data visualization.
        </p>
        <p class="text-center">
            <a href="https://github.com/anjonsunny/multi-modal-ai-reasoning-for-SA" class="btn btn-primary">View on GitHub</a>
        </p>
    </div>
</body>
</html>
